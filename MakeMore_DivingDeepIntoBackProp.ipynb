{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "w6Ld8_mQiCCN"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn.functional as F\n",
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# read in all the words\n",
        "words = open('/content/names.txt', 'r').read().splitlines()"
      ],
      "metadata": {
        "id": "BL2uzoX0wY1L"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# build the vocab of chars and mappings to/from intergers\n",
        "chars = sorted(list(set(''.join(words))))\n",
        "stoi = {s:i+1 for i, s in enumerate(chars)}\n",
        "stoi['.'] = 0\n",
        "itos = {i:s for s, i in stoi.items()}\n",
        "vocab_size = len(itos)\n",
        "print(itos)\n",
        "print(vocab_size)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "F9jL9jTGwpaV",
        "outputId": "fedade4f-ab49-4dca-f8a9-5985f04e40ad"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{1: 'a', 2: 'b', 3: 'c', 4: 'd', 5: 'e', 6: 'f', 7: 'g', 8: 'h', 9: 'i', 10: 'j', 11: 'k', 12: 'l', 13: 'm', 14: 'n', 15: 'o', 16: 'p', 17: 'q', 18: 'r', 19: 's', 20: 't', 21: 'u', 22: 'v', 23: 'w', 24: 'x', 25: 'y', 26: 'z', 0: '.'}\n",
            "27\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# build the data set\n",
        "block_size = 3 # context length: how many chars do we take to predict the next one\n",
        "def build_dataset(words):\n",
        "   X, Y = [], []\n",
        "\n",
        "   for w in words:\n",
        "    context = [0] * block_size\n",
        "    for ch in w + '.':\n",
        "      idx = stoi[ch]\n",
        "      X.append(context)\n",
        "      Y.append(idx)\n",
        "      context = context[1:] + [idx] # crop and append\n",
        "\n",
        "   X=torch.tensor(X)\n",
        "   Y=torch.tensor(Y)\n",
        "   print(X.shape, Y.shape)\n",
        "   return X, Y"
      ],
      "metadata": {
        "id": "67ek_b9Jwu94"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import random\n",
        "random.seed(42)\n",
        "random.shuffle(words)\n",
        "n1 = int(0.8*len(words))\n",
        "n2 = int(0.9*len(words))\n",
        "\n",
        "Xtr, Ytr = build_dataset(words[:n1])\n",
        "Xval, Yval = build_dataset(words[n1:n2])\n",
        "Xte, Yte = build_dataset(words[n2:])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "97LTUskgwv40",
        "outputId": "c1bc87ad-073c-4c3d-e103-fafec4fdcc45"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([182625, 3]) torch.Size([182625])\n",
            "torch.Size([22655, 3]) torch.Size([22655])\n",
            "torch.Size([22866, 3]) torch.Size([22866])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Utility function that will be used later when comparinf manual gradients to PyTorch gradients\n",
        "def cmp(s, dt, t):\n",
        "  ex = torch.all(dt == t.grad).item() # checking the gradients that we calculate with the gradients calculated by pytorch\n",
        "  app = torch.allclose(dt, t.grad)\n",
        "  maxdiff = (dt - t.grad).abs().max().item()\n",
        "  print(f'{s:15s} | exact: {str(ex):5s} | approximate {str(app):5s} | maxdiff: {maxdiff}')"
      ],
      "metadata": {
        "id": "XACncUdxwwPU"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from inspect import Parameter\n",
        "# Initilization\n",
        "n_embd = 10 # the dimensioanlity of the character embedding vectors\n",
        "n_hidden = 64 # the number of neurons in the hidden layer of MLP\n",
        "\n",
        "g = torch.Generator().manual_seed(2147483647)\n",
        "C = torch.randn((vocab_size, n_embd), generator=g)\n",
        "\n",
        "# layer 1\n",
        "W1 = torch.randn((n_embd * block_size, n_hidden), generator=g) * (5/3) / ((n_embd * block_size)**0.5) # multiplying by the gain and divinding by the fan-in\n",
        "b1 = torch.randn(n_hidden, generator=g) * 0.1 # might not be useful as we will use batch norm\n",
        "\n",
        "# layer 2\n",
        "W2 = torch.randn((n_hidden, vocab_size), generator=g) * 0.1\n",
        "b2 = torch.randn(vocab_size, generator=g) * 0.1\n",
        "\n",
        "# batch norm parameters\n",
        "bngain = torch.randn((1, n_hidden)) * 0.1 + 1.0\n",
        "bnbias = torch.randn((1, n_hidden)) * 0.1\n",
        "\n",
        "# Initialized many of the parameters in non-standard ways because sometimes initializing with eg. all zeros could mask an incorrect implementation of backward pass\n",
        "\n",
        "parameters = [C, W1, b1, W2, b2, bngain, bnbias]\n",
        "print(sum(p.nelement() for p in parameters))\n",
        "for p in parameters:\n",
        "  p.requires_grad = True\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kuPXg1i-xzHS",
        "outputId": "6518c37f-f573-4bab-d94b-8b20e7454bd7"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "4137\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "batch_size = 32\n",
        "\n",
        "#contruct minibatch\n",
        "idx = torch.randint(0, Xtr.shape[0], (batch_size, ), generator=g)\n",
        "Xb, Yb = Xtr[idx], Ytr[idx] # batch X, Y"
      ],
      "metadata": {
        "id": "Du798qBe0osb"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# forward pass\n",
        "\n",
        "emb = C[Xb] # # shape: (batch_size, block_size, n_embd)\n",
        "embcat = emb.view(emb.shape[0], -1)  # shape: (batch_size, block_size * n_embd)\n",
        "\n",
        "# linear layer 1\n",
        "hprebn = embcat @ W1 + b1 # shape: (batch_size, n_hidden), hidden layer pre-activation\n",
        "\n",
        "# batch norm layer\n",
        "bnmean_i = 1/batch_size*hprebn.sum(0, keepdim=True) # computes the mean of each hidden unit across the batch.\n",
        "bndiff = hprebn - bnmean_i # centers the activations | helps gradient flow and symmetry in weight updates.\n",
        "bndiff_sq = bndiff ** 2\n",
        "bnvar = 1/(batch_size-1)*(bndiff_sq).sum(0, keepdim=True) # to scale the activation | High variance can lead to unstable gradients.\n",
        "# NOTE: Bessel's correction (dividing by n-1, not n)\n",
        "bnvar_inv = (bnvar + 1e-5) ** -0.5 # inverse square root of variance for normalizing the scale of activations.\n",
        "bnraw = bndiff * bnvar_inv # normalizes the centered activations. Produces zero-mean, unit-variance output | Stabilizes training and improves convergence speed.\n",
        "hpreact = bngain * bnraw + bnbias # applies learnable scale (bngain) and shift (bnbias) to normalized activations | Prevents over-constraining the model; lets it learn richer representations.\n",
        "\n",
        "# non-linearity\n",
        "h = torch.tanh(hpreact) # hidden layer\n",
        "\n",
        "# linear layer 2\n",
        "logits = h @ W2 + b2 # output layer\n",
        "\n",
        "# cross entropy loss (same as F.cross_entropy(logits, Yb))\n",
        "logit_maxes = logits.max(1, keepdim=True).values\n",
        "norm_logits = logits - logit_maxes # subtract max for numerical stability\n",
        "# Doing the above step to make sure that the logits do not become to large values as we are going to exponentiate them\n",
        "counts = norm_logits.exp()\n",
        "counts_sum = counts.sum(1, keepdim=True)\n",
        "counts_sum_inv = counts_sum ** -1.0 # if we use (1.0/counts_sum) instead then we will not get exact backbrop values\n",
        "# We normalize in the above step to make sure the probs sum to 1\n",
        "probs = counts * counts_sum_inv # notice that the shapes are not same (32, 27) & (32, 1)\n",
        "# torch first broadcasts the values and then does the element wise multiplication\n",
        "logprobs = probs.log()\n",
        "loss = -logprobs[range(batch_size), Yb].mean()\n",
        "\n",
        "# Pytorch bacward pass\n",
        "for p in parameters:\n",
        "  p.grad = None\n",
        "\n",
        "for t in [logprobs, probs, counts, counts_sum, counts_sum_inv, norm_logits,\n",
        "          logit_maxes, logits, h, hpreact, bnraw, bnvar_inv, bnvar, bndiff_sq,\n",
        "          bndiff, hprebn, bnmean_i, embcat, emb]:\n",
        "  t.retain_grad()\n",
        "loss.backward()\n",
        "loss"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2NIgfjWD5U8S",
        "outputId": "d7dae592-b9ec-4787-a384-584acffe877a"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor(3.3586, grad_fn=<NegBackward0>)"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "'''\n",
        "loss = - (a + b + c)/3\n",
        "loss = -1/3a + -1/3b + -1/3c\n",
        "dloss/da = -1/n\n",
        "'''"
      ],
      "metadata": {
        "id": "B0ySPS4s-Eaa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "'''\n",
        "c = a * b, but with tensors\n",
        "a[3X3] * b [3X1]\n",
        "a11*b1 a12*b1 a13*b1\n",
        "a21*b2 a22*b2 a23*b2\n",
        "a31*b3 a32*b3 a33*b3\n",
        "'''"
      ],
      "metadata": {
        "id": "uMh9y8fXwf1K"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "'''\n",
        "counts.shape => (32, 27) and counts_sum.shape => (32, 1)\n",
        "\n",
        "a11 a12 a13 ---> b1 (= a11 + a12 + a13)\n",
        "a21 a22 a23 ---> b2 (= a21 + a22 + a23)\n",
        "a31 a32 a33 ---> b3 (= a31 + a32 + a33)\n",
        "'''"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "XA0ovLavEhxv",
        "outputId": "6697047c-2177-4c8b-ddac-564ab1423b67"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'\\ncounts.shape => (32, 27) and counts_sum.shape => (32, 1)\\n\\na11 a12 a13 ---> b1 (= a11 + a12 + a13)\\n'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 30
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "'''\n",
        "norm_logits.shape => (32, 27), logits.shape => (32, 27) & logit_maxes.shape => (32, 1)\n",
        "so again beware of broadcasting\n",
        "\n",
        "c11 c12 c13 = a11 a12 a13   b1\n",
        "c21 c22 c23 = a21 a22 a23 - b2\n",
        "c31 c32 c33 = a31 a32 a33   b3\n",
        "\n",
        "c32 = a32 - b3\n",
        "'''"
      ],
      "metadata": {
        "id": "GhlYpRrDHMkU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#dlogits.shape, h.shape, W2.shape, b2.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uEJfyP33Sg9i",
        "outputId": "d3dda742-f86e-4f26-c6e4-5b22c7c9d793"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(torch.Size([32, 27]),\n",
              " torch.Size([32, 64]),\n",
              " torch.Size([64, 27]),\n",
              " torch.Size([27]))"
            ]
          },
          "metadata": {},
          "execution_count": 38
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "'''\n",
        "bnvar = 1/(batch_size+1)*(bndiff_sq).sum(0, keepdim=True)\n",
        "a11, a12\n",
        "a21, a22\n",
        "\n",
        "b1, b2\n",
        "b1 = 1/(n-1)*(a11 + a21)\n",
        "b2 = 1/(n-1)*(a12 + a22)\n",
        "'''\n",
        "bnvar.shape, bndiff_sq.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BLoMN_VIxew1",
        "outputId": "833ebe17-976c-45d3-f5fc-cac2a22d2c24"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(torch.Size([1, 64]), torch.Size([32, 64]))"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Exercise 1: backprop through the while thing manually\n",
        "# backpropagating through exactly all of the variables as they are defined in the forward pass above, one by one\n",
        "\n",
        "dlogprobs = torch.zeros_like(logprobs) # create tensor in the shape of logprobs\n",
        "\n",
        "dlogprobs[range(batch_size), Yb] = -1.0/batch_size\n",
        "dprobs = (1.0 / probs) * dlogprobs # derivative of log(x) = 1/x, here x = probs\n",
        "dcounts_sum_inv = (counts * dprobs).sum(1, keepdim=True) # sum for the broadcasting neing done for counts_sum_inv\n",
        "dcounts = (counts_sum_inv * dprobs) # internally broadcasted\n",
        "dcounts_sum = (-1.0 / counts_sum ** 2) * dcounts_sum_inv\n",
        "dcounts += torch.ones_like(counts) * dcounts_sum\n",
        "dnorm_logits = norm_logits.exp() * dcounts # = counts * dcounts (value wise)\n",
        "dlogits = dnorm_logits.clone()\n",
        "dlogit_maxes = (-dlogits).sum(1, keepdim=True)\n",
        "dlogits += F.one_hot(logits.max(1).indices, num_classes=logits.shape[1]) * dlogit_maxes # can also be done in the way we calculated dlogprobs\n",
        "dh = dlogits @ W2.T\n",
        "dW2 = h.T @ dlogits\n",
        "db2 = dlogits.sum(0)\n",
        "dhpreact = (1.0 - h**2) * dh\n",
        "dbngain = (bnraw * dhpreact).sum(0, keepdim=True)\n",
        "dbnraw = bngain * dhpreact\n",
        "dbnbias = dhpreact.sum(0, keepdim=True)\n",
        "dbndiff = bnvar_inv * dbnraw\n",
        "dbnvar_inv = (bndiff * dbnraw).sum(0, keepdim=True)\n",
        "dbnvar = (-0.5*(bnvar + 1e-5)**-1.5) * dbnvar_inv\n",
        "dbndiff_sq = (1.0/(batch_size-1))*torch.ones_like(bndiff_sq) * dbnvar\n",
        "dbndiff += (2 * bndiff) * dbndiff_sq\n",
        "dhprebn = dbndiff.clone()\n",
        "dbnmean_i = ((-1.0)*dbndiff).sum(0)\n",
        "dhprebn += (1.0/batch_size) * (torch.ones_like(hprebn)) * dbnmean_i\n",
        "dembcat = dhprebn @ W1.T\n",
        "dW1 = embcat.T @ dhprebn\n",
        "db1 = dhprebn.sum(0)\n",
        "demb = dembcat.view(emb.shape)\n",
        "dC= torch.zeros_like(C)\n",
        "for i in range(Xb.shape[0]):\n",
        "  for j in range(Xb.shape[1]):\n",
        "    idx = Xb[i, j]\n",
        "    dC[idx] += demb[i, j]\n",
        "\n",
        "cmp('logprobs', dlogprobs, logprobs)\n",
        "cmp('logprobs', dprobs, probs)\n",
        "cmp('counts_sum_inv', dcounts_sum_inv, counts_sum_inv)\n",
        "cmp('counts_sum', dcounts_sum, counts_sum)\n",
        "cmp('counts', dcounts, counts)\n",
        "cmp('norm_logits', dnorm_logits, norm_logits)\n",
        "cmp('logit_maxes', dlogit_maxes, logit_maxes)\n",
        "cmp('logits', dlogits, logits)\n",
        "cmp('h', dh, h)\n",
        "cmp('W2', dW2, W2)\n",
        "cmp('b2', db2, b2)\n",
        "cmp('hpreact', dhpreact, hpreact)\n",
        "cmp('bngain', dbngain, bngain)\n",
        "cmp('bnraw', dbnraw, bnraw)\n",
        "cmp('bnbias', dbnbias, bnbias)\n",
        "cmp('bnvar_inv', dbnvar_inv, bnvar_inv)\n",
        "cmp('bnvar', dbnvar, bnvar)\n",
        "cmp('bndiff_sq', dbndiff_sq, bndiff_sq)\n",
        "cmp('bndiff', dbndiff, bndiff)\n",
        "cmp('hprebn', dhprebn, hprebn)\n",
        "cmp('bnmean_i', dbnmean_i, bnmean_i)\n",
        "cmp('embcat', dembcat, embcat)\n",
        "cmp('W1', dW1, W1)\n",
        "cmp('b1', db1, b1)\n",
        "cmp('emb', demb, emb)\n",
        "cmp('C', dC, C)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LBKJFFjdi6qa",
        "outputId": "994ba635-0f6f-473b-b5bf-314ae3e57b43"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "logprobs        | exact: True  | approximate True  | maxdiff: 0.0\n",
            "logprobs        | exact: True  | approximate True  | maxdiff: 0.0\n",
            "counts_sum_inv  | exact: True  | approximate True  | maxdiff: 0.0\n",
            "counts_sum      | exact: True  | approximate True  | maxdiff: 0.0\n",
            "counts          | exact: True  | approximate True  | maxdiff: 0.0\n",
            "norm_logits     | exact: True  | approximate True  | maxdiff: 0.0\n",
            "logit_maxes     | exact: True  | approximate True  | maxdiff: 0.0\n",
            "logits          | exact: True  | approximate True  | maxdiff: 0.0\n",
            "h               | exact: True  | approximate True  | maxdiff: 0.0\n",
            "W2              | exact: True  | approximate True  | maxdiff: 0.0\n",
            "b2              | exact: True  | approximate True  | maxdiff: 0.0\n",
            "hpreact         | exact: False | approximate True  | maxdiff: 4.656612873077393e-10\n",
            "bngain          | exact: False | approximate True  | maxdiff: 1.862645149230957e-09\n",
            "bnraw           | exact: False | approximate True  | maxdiff: 9.313225746154785e-10\n",
            "bnbias          | exact: False | approximate True  | maxdiff: 3.725290298461914e-09\n",
            "bnvar_inv       | exact: False | approximate True  | maxdiff: 3.725290298461914e-09\n",
            "bnvar           | exact: False | approximate True  | maxdiff: 6.984919309616089e-10\n",
            "bndiff_sq       | exact: False | approximate True  | maxdiff: 2.1827872842550278e-11\n",
            "bndiff          | exact: False | approximate True  | maxdiff: 9.313225746154785e-10\n",
            "hprebn          | exact: False | approximate True  | maxdiff: 9.313225746154785e-10\n",
            "bnmean_i        | exact: False | approximate True  | maxdiff: 1.862645149230957e-09\n",
            "embcat          | exact: False | approximate True  | maxdiff: 1.3969838619232178e-09\n",
            "W1              | exact: False | approximate True  | maxdiff: 3.725290298461914e-09\n",
            "b1              | exact: False | approximate True  | maxdiff: 5.587935447692871e-09\n",
            "emb             | exact: False | approximate True  | maxdiff: 1.3969838619232178e-09\n",
            "C               | exact: False | approximate True  | maxdiff: 1.1175870895385742e-08\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Exercise 2: backprop through cross_entropy but all in one go\n",
        "\n",
        "# forward pass\n",
        "\n",
        "'''\n",
        "before:\n",
        "logit_maxes = logits.max(1, keepdim=True).values\n",
        "norm_logits = logits - logit_maxes # subtract max for numerical stability\n",
        "# Doing the above step to make sure that the logits do not become to large values as we are going to exponentiate them\n",
        "counts = norm_logits.exp()\n",
        "counts_sum = counts.sum(1, keepdim=True)\n",
        "counts_sum_inv = counts_sum ** -1.0 # if we use (1.0/counts_sum) instead then we will not get exact backbrop values\n",
        "# We normalize in the above step to make sure the probs sum to 1\n",
        "probs = counts * counts_sum_inv # notice that the shapes are not same (32, 27) & (32, 1)\n",
        "# torch first broadcasts the values and then does the element wise multiplication\n",
        "logprobs = probs.log()\n",
        "loss = -logprobs[range(batch_size), Yb].mean()\n",
        "'''\n",
        "\n",
        "# now:\n",
        "loss_fast = F.cross_entropy(logits, Yb)\n",
        "print(loss_fast.item(), 'diff', (loss_fast-loss).item())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LV3c95wclON4",
        "outputId": "bd237cc8-7993-4217-f2e6-469341f675ca"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "3.358550548553467 diff -4.76837158203125e-07\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# backward pass\n",
        "\n",
        "# derivative is p_i if i!=y else p_i-1\n",
        "dlogits = F.softmax(logits, 1) # we can to do the softmax along the rows of the logits\n",
        "dlogits[range(batch_size), Yb] -= 1 # for the elements that are at Yb subtract -1\n",
        "dlogits /= batch_size # we need the average loss for the batch\n",
        "\n",
        "cmp('logits', dlogits, logits)"
      ],
      "metadata": {
        "id": "sznHglR06u3Z",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4d174537-fe30-48f9-898d-db3a3794073b"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "logits          | exact: False | approximate True  | maxdiff: 6.51925802230835e-09\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "F.softmax(logits, 1)[0]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Avho5Bacs1uR",
        "outputId": "dbbaf16e-1c7b-4bff-edcc-74a942f7e1d8"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([0.0703, 0.0898, 0.0195, 0.0465, 0.0206, 0.0769, 0.0254, 0.0365, 0.0178,\n",
              "        0.0297, 0.0378, 0.0406, 0.0401, 0.0300, 0.0359, 0.0124, 0.0098, 0.0190,\n",
              "        0.0172, 0.0530, 0.0495, 0.0195, 0.0297, 0.0712, 0.0529, 0.0266, 0.0216],\n",
              "       grad_fn=<SelectBackward0>)"
            ]
          },
          "metadata": {},
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# dlogits is the probs matrix in the forward pass\n",
        "# multiplying by batch_size so that we do not have the scaling by batch_size (avg)\n",
        "dlogits[0]*batch_size # the place where we subtracted 1, so the correct indices"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VDhquLGCuCol",
        "outputId": "c40ba8d8-300b-4ea9-e19b-10a71463c014"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([ 0.0703,  0.0898,  0.0195,  0.0465,  0.0206,  0.0769,  0.0254,  0.0365,\n",
              "        -0.9822,  0.0297,  0.0378,  0.0406,  0.0401,  0.0300,  0.0359,  0.0124,\n",
              "         0.0098,  0.0190,  0.0172,  0.0530,  0.0495,  0.0195,  0.0297,  0.0712,\n",
              "         0.0529,  0.0266,  0.0216], grad_fn=<MulBackward0>)"
            ]
          },
          "metadata": {},
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "dlogits[0].sum() # these sum up to zero -> the value maybe somethines be very to close to zero, python decimal calculation...."
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Q_jyuTKEwYzl",
        "outputId": "029e88b2-28a9-410f-c964-a88e931693e2"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor(-2.7940e-09, grad_fn=<SumBackward0>)"
            ]
          },
          "metadata": {},
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "'''\n",
        "the gradient at each cell is like a force where we try to push the correct probs up and pull the incorrect probs down,\n",
        "and the amount of force applied to push and pull the probs sums to zero\n",
        "the amount of force we apply to propotional to the probs that come out in the forward pass\n",
        "\n",
        "if the probs were exactly correct then in that case,\n",
        "we will have zero at all incorrect indices\n",
        "and 1 at the correct index\n",
        "and then in this cae dlogits will be all zero (0 by default at incorrect, and we subtract 1 for the correct, so it becomes zero as well)\n",
        "'''"
      ],
      "metadata": {
        "id": "Hik7MxqXxHEQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plt.figure(figsize=(8,8))\n",
        "plt.imshow(dlogits.detach(), cmap='gray')\n",
        "# the black spots are the correct indices"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 693
        },
        "id": "nx4i41SmzDcd",
        "outputId": "1cfe8b43-4a93-4c59-f59b-bf068e0f77f1"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<matplotlib.image.AxesImage at 0x7d17ebf80390>"
            ]
          },
          "metadata": {},
          "execution_count": 17
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 800x800 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Exercise 3: backprop through batchnorm but all in one go\n",
        "\n",
        "# forward pass\n",
        "\n",
        "'''\n",
        "before:\n",
        "\n",
        "# batch norm layer\n",
        "bnmean_i = 1/batch_size*hprebn.sum(0, keepdim=True) # computes the mean of each hidden unit across the batch.\n",
        "bndiff = hprebn - bnmean_i # centers the activations | helps gradient flow and symmetry in weight updates.\n",
        "bndiff_sq = bndiff ** 2\n",
        "bnvar = 1/(batch_size+1)*(bndiff_sq).sum(0, keepdim=True) # to scale the activation | High variance can lead to unstable gradients.\n",
        "# NOTE: Bessel's correction (dividing by n-1, not n)\n",
        "bnvar_inv = (bnvar + 1e-5) ** -0.5 # inverse square root of variance for normalizing the scale of activations.\n",
        "bnraw = bndiff * bnvar_inv # normalizes the centered activations. Produces zero-mean, unit-variance output | Stabilizes training and improves convergence speed.\n",
        "hpreact = bngain * bnraw + bnbias # applies learnable scale (bngain) and shift (bnbias) to normalized activations | Prevents over-constraining the model; lets it learn richer representations.\n",
        "'''\n",
        "\n",
        "# now:\n",
        "\n",
        "hpreact_fast = bngain * (hprebn - hprebn.mean(0, keepdim=True)) / torch.sqrt(hprebn.var(0, keepdim=True, unbiased=True) + 1e-5) + bnbias\n",
        "print('max diff:', (hpreact_fast - hpreact).abs().max())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BT_9QpAezPW2",
        "outputId": "191d94b9-fe34-4230-ed9e-1abbc1304f18"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "max diff: tensor(4.7684e-07, grad_fn=<MaxBackward1>)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# backward pass\n",
        "\n",
        "'''\n",
        "before:\n",
        "\n",
        "dbnraw = bngain * dhpreact\n",
        "dbnbias = dhpreact.sum(0, keepdim=True)\n",
        "dbndiff = bnvar_inv * dbnraw\n",
        "dbnvar_inv = (bndiff * dbnraw).sum(0, keepdim=True)\n",
        "dbnvar = (-0.5*(bnvar + 1e-5)**-1.5) * dbnvar_inv\n",
        "dbndiff_sq = (1.0/(batch_size-1))*torch.ones_like(bndiff_sq) * dbnvar\n",
        "dbndiff += (2 * bndiff) * dbndiff_sq\n",
        "dhprebn = dbndiff.clone()\n",
        "dbnmean_i = ((-1.0)*dbndiff).sum(0)\n",
        "dhprebn += (1.0/batch_size) * (torch.ones_like(hprebn)) * dbnmean_i\n",
        "'''\n",
        "\n",
        "#now:\n",
        "dhprebn = ((bngain * bnvar_inv) / batch_size) * (batch_size * dhpreact - dhpreact.sum(0) - batch_size/(batch_size - 1) * bnraw * (dhpreact * bnraw).sum(0))\n",
        "cmp('hprebn', dhprebn, hprebn)"
      ],
      "metadata": {
        "id": "E0ukv4ky3P0r",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a4671734-6cac-47ff-e0f8-eab67d03fe12"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "hprebn          | exact: False | approximate True  | maxdiff: 9.313225746154785e-10\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Exercise 4: Putting it all together\n",
        "# Train the MLP neura; net with our own backward pass\n",
        "\n",
        "n_embd = 10 # the dimensionality of the character embedding the vectors\n",
        "n_hidden = 200 # the number of neurons in the hidden layer of the MLP\n",
        "\n",
        "g = torch.Generator().manual_seed(2147483647)\n",
        "C = torch.randn((vocab_size, n_embd), generator=g)\n",
        "\n",
        "# layer 1\n",
        "W1 = torch.randn((n_embd * block_size, n_hidden), generator=g) * (5/3) / ((n_embd * block_size) ** 0.5)\n",
        "b1 = torch.randn(n_hidden, generator=g) * 0.1\n",
        "\n",
        "# layer 2\n",
        "W2 = torch.randn((n_hidden, vocab_size), generator=g) * 0.1\n",
        "b2 = torch.randn(vocab_size, generator=g) * 0.1\n",
        "\n",
        "# batch norm parameters\n",
        "bngain = torch.randn((1, n_hidden)) * 0.1 + 1.0\n",
        "bnbias = torch.randn((1, n_hidden)) * 0.1\n",
        "\n",
        "parameters = [C, W1, b1, W2, b2, bngain, bnbias]\n",
        "print(sum(p.nelement() for p in parameters))\n",
        "for p in parameters:\n",
        "  p.requires_grad = True\n",
        "\n",
        "# same optimization as last time\n",
        "max_steps = 200000\n",
        "batch_size = 32\n",
        "loss_i = []\n",
        "\n",
        "# use the following when not useing loss.backward\n",
        "with torch.no_grad():\n",
        "\n",
        "  # kick of optimization\n",
        "  for i in range(max_steps):\n",
        "\n",
        "    # mini batch construct\n",
        "    idx = torch.randint(0, Xtr.shape[0], (batch_size,), generator=g)\n",
        "    Xb, Yb = Xtr[idx], Ytr[idx]\n",
        "\n",
        "    # forward pass\n",
        "    emb = C[Xb] # embed the vectors\n",
        "    embcat = emb.view(emb.shape[0], -1) # concat the vectors\n",
        "    # linear layer\n",
        "    hprebn = embcat @ W1 + b1 # hidden layer pre activation\n",
        "    # batch norm layer\n",
        "    #---------------------------------------------------------------------------\n",
        "    bnmean = hprebn.mean(0, keepdim=True)\n",
        "    bnvar = hprebn.var(0, keepdim=True, unbiased = True)\n",
        "    bnvar_inv = (bnvar + 1e-5) ** -0.5 # inv and sqrt\n",
        "    bnraw = (hprebn - bnmean) * bnvar_inv\n",
        "    hpreact = bngain * bnraw + bnbias\n",
        "    #---------------------------------------------------------------------------\n",
        "    # non-leniarity\n",
        "    h = torch.tanh(hpreact) # hidden layer\n",
        "    logits = h @ W2 + b2 # output layer\n",
        "    loss = F.cross_entropy(logits, Yb) # loss function\n",
        "\n",
        "    # backward pass\n",
        "    for p in parameters:\n",
        "      p.grad = None\n",
        "    #loss.backward() # uncomment this when not using manual backprop or for verification of manual answer\n",
        "\n",
        "    # manual backprop!!! -------------------------------------------------------\n",
        "    dlogits = F.softmax(logits, 1)\n",
        "    dlogits[range(batch_size), Yb] -= 1\n",
        "    dlogits /= batch_size\n",
        "    # 2nd layer\n",
        "    dh = dlogits @ W2.T\n",
        "    dW2 = h.T @ dlogits\n",
        "    db2 = dlogits.sum(0)\n",
        "    # for tanh\n",
        "    dhpreact = (1.0 - h**2) * dh\n",
        "    # batch norm layer\n",
        "    dbngain = (bnraw * dhpreact).sum(0, keepdim=True)\n",
        "    dbnbias = dhpreact.sum(0, keepdim=True)\n",
        "    dhprebn =  (bngain * bnvar_inv / batch_size) * (batch_size * dhpreact - dhpreact.sum(0) - batch_size/(batch_size - 1) * bnraw * (dhpreact * bnraw).sum(0))\n",
        "    # 1st layer\n",
        "    dembcat = dhprebn @ W1.T\n",
        "    dW1 = embcat.T @ dhprebn\n",
        "    db1 = dhprebn.sum(0)\n",
        "    # for embedding part\n",
        "    demb = dembcat.view(emb.shape)\n",
        "    dC = torch.zeros_like(C)\n",
        "    for a in range(Xb.shape[0]):\n",
        "      for b in range(Xb.shape[1]):\n",
        "        c = Xb[a, b]\n",
        "        dC[c] += demb[a, b]\n",
        "    grads = [dC, dW1, db1, dW2, db2, dbngain, dbnbias] #-------------------------\n",
        "\n",
        "\n",
        "    # update\n",
        "    lr = 0.1 if i < 100000 else 0.01 # step learning rate decay\n",
        "    for p, grad in zip(parameters, grads):\n",
        "      #p.data += -lr * p.grad # old way of cheems doge (using PyTorch grad from .backward())\n",
        "      p.data += -lr * grad # new way of swole doge TODO: enable\n",
        "\n",
        "    # track stats\n",
        "    if i % 10000 == 0: # print every once in a while\n",
        "      print(f'{i:7d}/{max_steps:7d}: {loss.item():.4f}')\n",
        "    loss_i.append(loss.log10().item())\n",
        "\n",
        "    #if i >= 100: # TODO: delete early breaking when you're ready to train the full net\n",
        "    #  break"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-2T-ASNoATp_",
        "outputId": "43b883ba-045c-4cea-89ba-66ffbd157f2f"
      },
      "execution_count": 62,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "12297\n",
            "      0/ 200000: 3.7294\n",
            "  10000/ 200000: 2.1764\n",
            "  20000/ 200000: 2.3869\n",
            "  30000/ 200000: 2.5004\n",
            "  40000/ 200000: 1.9757\n",
            "  50000/ 200000: 2.3242\n",
            "  60000/ 200000: 2.3282\n",
            "  70000/ 200000: 1.9698\n",
            "  80000/ 200000: 2.2784\n",
            "  90000/ 200000: 2.2067\n",
            " 100000/ 200000: 1.9002\n",
            " 110000/ 200000: 2.3915\n",
            " 120000/ 200000: 1.9416\n",
            " 130000/ 200000: 2.4663\n",
            " 140000/ 200000: 2.2794\n",
            " 150000/ 200000: 2.0789\n",
            " 160000/ 200000: 1.8816\n",
            " 170000/ 200000: 1.8242\n",
            " 180000/ 200000: 1.9948\n",
            " 190000/ 200000: 1.9407\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# for checking the gradients\n",
        "for p, g in zip(parameters, grads):\n",
        "  cmp(str(tuple(p.shape)), g, p)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4aSQ9TXzOnRL",
        "outputId": "fefa0e3b-b77d-4ea2-9e4f-a94a63da0931"
      },
      "execution_count": 58,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(27, 10)        | exact: False | approximate True  | maxdiff: 1.30385160446167e-08\n",
            "(30, 200)       | exact: False | approximate True  | maxdiff: 7.916241884231567e-09\n",
            "(200,)          | exact: False | approximate True  | maxdiff: 5.587935447692871e-09\n",
            "(200, 27)       | exact: False | approximate True  | maxdiff: 1.4901161193847656e-08\n",
            "(27,)           | exact: False | approximate True  | maxdiff: 7.450580596923828e-09\n",
            "(1, 200)        | exact: False | approximate True  | maxdiff: 3.026798367500305e-09\n",
            "(1, 200)        | exact: False | approximate True  | maxdiff: 7.450580596923828e-09\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# calliberate the batch norm at the end of the training\n",
        "\n",
        "with torch.no_grad():\n",
        "  # pass the training through\n",
        "  emb = C[Xtr]\n",
        "  embcat = emb.view(emb.shape[0], -1)\n",
        "  hpreact = embcat @ W1 + b1\n",
        "  # measure the mean, std dev over the entire training set\n",
        "  bnmean = hpreact.mean(0, keepdim=True)\n",
        "  bnvar = hpreact.var(0, keepdim=True)"
      ],
      "metadata": {
        "id": "GyVMg28XOnrS"
      },
      "execution_count": 63,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# evaluate train and val loss\n",
        "\n",
        "@torch.no_grad() # disable the gradient tracking\n",
        "def split_loss(split):\n",
        "  x, y = {\n",
        "      'train': {Xtr, Ytr},\n",
        "      'val': {Xval, Yval},\n",
        "      'test': {Xte, Yte},\n",
        "  }[split]\n",
        "\n",
        "  emb = C[x] # (batch_size, block_size, n_embd)\n",
        "  embcat = emb.view(emb.shape[0], -1) # concat to (batch_size, block_size * n_embd)\n",
        "  hpreact = embcat @ W1 + b1\n",
        "  hpreact = bngain * (hpreact - bnmean) * (bnvar + 1e-5) ** -0.5 + bnbias\n",
        "  h = torch.tanh(hpreact)\n",
        "  logits = h @ W2 + b2\n",
        "  loss = F.cross_entropy(logits, y)\n",
        "  print(split, loss.item())\n",
        "\n",
        "split_loss('train')\n",
        "split_loss('val')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nLyPqhbnOn_5",
        "outputId": "9e46d93b-03c0-4b3b-d9c9-e21abeae9987"
      },
      "execution_count": 64,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "train 2.071078300476074\n",
            "val 2.110525369644165\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# sample from the model\n",
        "g = torch.Generator().manual_seed(2147483647 + 10)\n",
        "\n",
        "for _ in range(20):\n",
        "\n",
        "    out = []\n",
        "    context = [0] * block_size # initialize with all ...\n",
        "    while True:\n",
        "      # forward pass\n",
        "      emb = C[torch.tensor([context])] # (1, block_size, d)\n",
        "      embcat = emb.view(emb.shape[0], -1) # concat into (batch_size, block_size * n_embd)\n",
        "      hpreact = embcat @ W1 + b1\n",
        "      hpreact = bngain * (hpreact - bnmean) * (bnvar + 1e-5) ** -0.5 + bnbias\n",
        "      h = torch.tanh(hpreact) # (N, n_hidden)\n",
        "      logits = h @ W2 + b2 # (N, vocab_size)\n",
        "      # sample\n",
        "      probs = F.softmax(logits, dim=1)\n",
        "      ix = torch.multinomial(probs, num_samples=1, generator=g).item()\n",
        "      context = context[1:] + [ix]\n",
        "      out.append(ix)\n",
        "      if ix == 0:\n",
        "        break\n",
        "\n",
        "    print(''.join(itos[i] for i in out))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "42FKup1eQgIY",
        "outputId": "b6790d52-64ae-4e6c-d8f5-665c931725dc"
      },
      "execution_count": 65,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "mora.\n",
            "mayah.\n",
            "see.\n",
            "mad.\n",
            "rylle.\n",
            "emman.\n",
            "endraegusteredielin.\n",
            "shi.\n",
            "jen.\n",
            "eden.\n",
            "estanar.\n",
            "kayzion.\n",
            "kamin.\n",
            "shubergshirael.\n",
            "kindreelle.\n",
            "jose.\n",
            "casubented.\n",
            "ryyah.\n",
            "faeh.\n",
            "yuma.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "thBe-_ZOUuqY"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}